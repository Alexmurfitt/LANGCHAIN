# Informe auto-guiado: de cero a tu propio Gemini ChatBox con LangChain
Este resumen integra todo lo que hemos cubierto, organizado en un orden l√≥gico para que puedas reproducirlo sin perder detalle.

# 1. Ecosistema LangChain ‚Äî ¬øqui√©n es qui√©n?
Herramienta	Idea en una frase	Cu√°ndo la usas
LangChain	Framework base para orquestar LLMs (prompts, chains, agentes).	Cualquier MVP con un modelo de lenguaje.
LangGraph	Capa sobre LangChain que modela tu aplicaci√≥n como un grafo: nodos, bucles, estado, m√∫ltiples agentes.	Flujos complejos y sistemas multi-agente.
LangSmith	Observabilidad: trazas, depuraci√≥n, pruebas A/B y m√©tricas.	Monitorizar y mejorar apps basadas en LangChain/LangGraph.

Resumen ejecutivo

Construyes con LangChain ‚Üí orquestas flujos avanzados con LangGraph ‚Üí mides y depuras con LangSmith.

# 2. Por qu√© las empresas lo exigen
Tecnolog√≠a puntera: LLMs son ahora ‚Äúlenguaje com√∫n‚Äù en IA; LangChain es el est√°ndar de facto.

Ahorro de tiempo: Abstrae prompts, memoria, conexiones a vectores/DB.

Escalabilidad y fiabilidad: LangSmith ofrece trazabilidad y m√©tricas listas para producci√≥n.

Integraci√≥n nativa con Azure/OpenAI/Google: encaja bien en pilas corporativas.

(MCP ‚Äì Microsoft Certified Professional‚Äì aporta sinergia si tu empresa despliega IA en Azure OpenAI Service.)

# 3. Entorno Python listo para LLMs
bash
Copiar
Editar
# 1 ‚Äî Crear entorno
python -m venv myenv
source myenv/bin/activate   # Windows: myenv\Scripts\activate

# 2 ‚Äî Instalar dependencias
pip install -U langchain langchain-google-genai google-generativeai python-dotenv
.env m√≠nimo:

GOOGLE_API_KEY=TU_CLAVE_DE_GOOGLE
LANGSMITH_API_KEY=TU_CLAVE_LANGSMITH   # opcional

# 4. Script inicial y errores t√≠picos
4.1 C√≥digo base (bloque que pediste)

import os
from dotenv import load_dotenv

load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

os.environ["LANGSMITH_TRACING"] = os.getenv("LANGSMITH_TRACING", "true")
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "tu_api_key")
os.environ["LANGSMITH_PROJECT"] = os.getenv("LANGSMITH_PROJECT", "default")
4.2 Error NameError: ChatGoogleGenerativeAI
Causa ‚Üí no se import√≥ la clase ni el paquete estaba instalado.
Soluci√≥n ‚Üí instalar langchain-google-genai y a√±adir:


from langchain_google_genai import ChatGoogleGenerativeAI
4.3 Error 404 / 429 (modelo)
Usar un ID de modelo v√°lido.

Ej.: gemini-1.5-flash (m√°s barato/r√°pido)

gemini-1.5-pro-latest si tienes cuota.

# 5. Versi√≥n sin advertencias (API ‚â• LangChain 0.2.7)

import os
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnableLambda
from langchain.memory import ConversationBufferMemory
from langchain_core.chat_history import InMemoryChatMessageHistory

load_dotenv()
google_api_key = os.environ["GOOGLE_API_KEY"]

# Configuraci√≥n LangSmith opcional
os.environ.setdefault("LANGSMITH_TRACING", "true")
os.environ.setdefault("LANGSMITH_PROJECT", "default")

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key=google_api_key,
    temperature=0.3,
)

def get_memory(_session_id: str):
    return ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        chat_memory=InMemoryChatMessageHistory()
    )

chain = RunnableWithMessageHistory(
    RunnableLambda(lambda kv: llm.invoke(kv["input"])),
    get_memory,
    input_messages_key="input",
    history_messages_key="chat_history"
)

print("\nüß† Gemini ChatBox ‚Äî escribe 'exit' para salir\n")
session_id = "default"

while True:
    user = input("T√∫: ").strip()
    if user.lower() in {"exit", "quit"}:
        break
    try:
        res = chain.invoke({"input": user}, config={"configurable": {"session_id": session_id}})
        print("ü§ñ:", res.content, "\n")
    except Exception as e:
        print("‚ö†Ô∏è Error:", e, "\n")
Ventajas

Usa RunnableWithMessageHistory ‚Üí sin deprecaciones.

Memoria en RAM f√°cilmente reemplazable por Redis/DB.

Compatible con LangSmith para trazas de producci√≥n.

# 6. Checklist para reproducir
Paso	Acci√≥n	Resultado
1	Crear/activar venv	Entorno aislado
2	pip install ...	Dependencias listas
3	.env con GOOGLE_API_KEY (y LangSmith si quieres)	Variables accesibles
4	Copiar el script final	ChatBox limpio
5	Ejecutar python main.py	Conversar con Gemini
6	(Opc.) Abrir LangSmith	Ver trazas, latencia, coste

# 7. Pr√≥ximos pasos sugeridos
Persistencia de memoria

Cambia InMemoryChatMessageHistory() por Redis o Postgres si necesitas historial entre sesiones.

RAG / b√∫squeda sem√°ntica

A√±ade un Retriever (Chroma, Pinecone‚Ä¶) en la cadena para incorporar contexto.

Observabilidad completa

Configura LANGSMITH_API_KEY y explora el dashboard.

Despliegue r√°pido

Convierte el script en endpoint FastAPI o en Streamlit para demo interna.

# Conclusi√≥n
LangChain te da la base, LangGraph complejiza flujos y LangSmith te muestra qu√© pasa por dentro.

Importa siempre la clase correcta y usa IDs v√°lidos de modelo Gemini.

Desde LangChain 0.2.7 migra a RunnableWithMessageHistory para evitar deprecaciones.

Con ~50 l√≠neas de c√≥digo tienes un chat operativo, trazable y listo para escalar.

Con este informe ya puedes reconstruir todo el proceso de principio a fin y entender por qu√© cada paso es necesario. 


# üß† Gu√≠a Definitiva de LangChain: De Principiante a Experto

Esta gu√≠a combina la claridad pedag√≥gica de una introducci√≥n progresiva con la robustez de un entorno profesional. Si la dominas, estar√°s capacitado para dise√±ar, implementar y escalar sistemas avanzados con modelos de lenguaje (LLMs) usando LangChain, LangGraph y LangSmith.

---

## 1. üåê Introducci√≥n: Ecosistema LangChain

| Herramienta   | Rol                                                                    |
| ------------- | ---------------------------------------------------------------------- |
| **LangChain** | Framework base para orquestar LLMs, prompts, cadenas, agentes.         |
| **LangGraph** | Modelo de aplicaciones como grafos con estado y agentes colaborativos. |
| **LangSmith** | Observabilidad, trazas, pruebas A/B, evaluaci√≥n y depuraci√≥n.          |

> **Resumen:** Construyes con LangChain ‚Üí orquestas flujos con LangGraph ‚Üí supervisas con LangSmith.

---

## 2. ‚ö° Por qu√© es fundamental para empresas y expertos

* **Est√°ndar de facto** para apps basadas en LLMs.
* **Abstrae** detalles t√©cnicos: prompts, APIs, DBs, vectores.
* **Escalable**: soporta agentes, memoria, RAG y despliegue.
* **Trazable**: integra LangSmith para ver lo que hace tu modelo.
* **Compatible con** OpenAI, Gemini, Claude, Ollama, Azure, Pinecone, Redis, Chroma...

---

## 3. üìÑ Preparaci√≥n del entorno (Python)

```bash
# 1. Crear entorno virtual
python3 -m venv myenv
source myenv/bin/activate     # En Windows: myenv\Scripts\activate

# 2. Instalar dependencias
pip install -U langchain langchain-google-genai google-generativeai python-dotenv
```

### .env (variables secretas)

```
GOOGLE_API_KEY=TU_CLAVE_REAL
LANGSMITH_API_KEY=OPCIONAL
LANGSMITH_PROJECT=default
```

---

## 4. üëÅÔ∏è Fundamentos t√©cnicos de LangChain

### ‚úÖ Componentes esenciales

* **LLM**: motor de lenguaje (OpenAI, Gemini, Claude, Ollama...)
* **PromptTemplate**: estructura para dar instrucciones al modelo.
* **Memory**: almacena interacciones para dar contexto.
* **Chains**: secuencias de pasos conectados (prompt ‚Üí modelo ‚Üí salida).
* **Agents**: decisiones din√°micas seg√∫n herramientas/entorno.

---

## 5. üîß Proyecto inicial: Gemini ChatBox con LangChain

### 5.1 üé® C√≥digo educativo (para comprender los bloques)

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=google_api_key,
    temperature=0.3
)

memory = ConversationBufferMemory(return_messages=True)
chat_chain = ConversationChain(llm=llm, memory=memory)

print("\nüß† Gemini ChatBox ‚Äî escribe 'exit' para salir\n")
while True:
    user_input = input("T√∫: ").strip()
    if user_input.lower() in {"exit", "quit"}:
        break
    response = chat_chain.predict(input=user_input)
    print("ü§ñ:", response, "\n")
```

> ‚ö†√æ `ConversationChain` est√° **obsoleta** desde LangChain `0.2.7`.

---

### 5.2 üîÑ C√≥digo moderno con `RunnableWithMessageHistory`

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnableLambda
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain.memory import ConversationBufferMemory
import os
from dotenv import load_dotenv

load_dotenv()
google_api_key = os.environ["GOOGLE_API_KEY"]

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=google_api_key,
    temperature=0.3
)

def get_memory(session_id):
    return ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        chat_memory=InMemoryChatMessageHistory()
    )

chain = RunnableWithMessageHistory(
    RunnableLambda(lambda kv: llm.invoke(kv["input"])),
    get_memory,
    input_messages_key="input",
    history_messages_key="chat_history"
)

session_id = "usuario1"
print("\nüß† Gemini ChatBox (moderno)\n")
while True:
    user = input("T√∫: ").strip()
    if user.lower() in {"exit", "quit"}:
        break
    res = chain.invoke({"input": user}, config={"configurable": {"session_id": session_id}})
    print("ü§ñ:", res.content, "\n")
```

---

## 6. üîí Memoria en LangChain

| Tipo                              | Uso                                         |
| --------------------------------- | ------------------------------------------- |
| `ConversationBufferMemory`        | Guarda todo el historial de la conversaci√≥n |
| `ConversationSummaryBufferMemory` | Resume interacciones largas                 |
| `ConversationTokenBufferMemory`   | Limita memoria seg√∫n n√∫mero de tokens       |
| `RunnableWithMessageHistory`      | Moderna, flexible y escalable               |

> Sustituye `ConversationChain` por `RunnableWithMessageHistory` si usas LangChain ‚â• 0.2.7

---

## 7. üîç LangSmith: observabilidad profesional

Configura en `.env`:

```
LANGSMITH_API_KEY=tu_api_key
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=default
```

Beneficios:

* Visualiza prompts, entradas y salidas.
* Eval√∫a rendimiento y coste.
* Pruebas A/B de modelos.
* Diagn√≥stico de errores.

---

## 8. ü™Ñ Escalabilidad y siguientes pasos

| Siguiente Nivel              | Qu√© te permite hacer                                                              |
| ---------------------------- | --------------------------------------------------------------------------------- |
| ‚úî Persistencia de memoria    | Usa Redis o base de datos para guardar historial real de usuarios                 |
| ‚úî Integraci√≥n con RAG        | Recupera contexto desde PDFs, Web, vectores (FAISS, Pinecone...)                  |
| ‚úî Agents y Tools             | Actores inteligentes que deciden qu√© acci√≥n ejecutar (b√∫squeda, c√°lculo, APIs...) |
| ‚úî LangGraph                  | Dise√±a apps como flujos de estados, nodos, ciclos y agentes cooperativos          |
| ‚úî Deploy como API o interfaz | Convierte tu agente en una API FastAPI o app visual en Streamlit                  |

---

## üåü Conclusi√≥n

LangChain no solo es una biblioteca, es una **arquitectura completa** para construir sistemas avanzados con lenguaje natural. Dominarla implica:

1. Comprender sus componentes.
2. Usar su versi√≥n moderna sin deprecaciones.
3. Controlar el ciclo completo: input ‚Üí LLM ‚Üí respuesta ‚Üí trazabilidad.
4. Escalar con LangGraph y LangSmith.

Con esta gu√≠a puedes no solo crear un MVP funcional, sino tambi√©n escalarlo hacia un sistema profesional de IA conversacional, trazable y robusto.

