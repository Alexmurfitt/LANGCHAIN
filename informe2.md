# üß† Gu√≠a Definitiva de LangChain, LangGraph & LangSmith

**De cero a experto en desarrollo de aplicaciones con LLMs**

---

## 0. Visi√≥n global del ecosistema

| Herramienta   | Rol principal                                                       | Cu√°ndo la usas                                                             |
| ------------- | ------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| **LangChain** | Framework base para orquestar LLMs, promts, tools, memory & agents. | Siempre que quieras un MVP, RAG, chat o agente sobre un LLM.               |
| **LangGraph** | Capa de control de flujo basada en grafos (nodos + estado + loops). | Flujos complejos, multi‚Äëagente, recuperaci√≥n de contexto avanzado.         |
| **LangSmith** | Observabilidad (trazas, m√©tricas, A/B, evaluaci√≥n).                 | Cuando tu app sale de pruebas y necesitas medir coste, latencia y calidad. |

> **Pipeline ideal**: dise√±as con **LangChain**, escalas con **LangGraph** y monitorizas con **LangSmith**.

---

## 1. Fundamentos de LangChain

### 1.1 Componentes clave

| Componente         | Descripci√≥n breve                                                       |
| ------------------ | ----------------------------------------------------------------------- |
| **LLM**            | Motor de lenguaje (OpenAI, Gemini, Claude, Ollama‚Ä¶).                    |
| **PromptTemplate** | Plantilla parametrizable para generar instrucciones.                    |
| **Memory**         | Guarda contexto entre turnos (Buffer, Window, Summary, Token).          |
| **Tool**           | Funci√≥n externa que el modelo puede invocar (APIs, c√≥digo Python, DB‚Ä¶). |
| **Chain**          | Secuencia de pasos LLM/Tool; puede ser lineal o condicional.            |
| **Agent**          | Entidad que decide din√°micamente qu√© herramienta usar.                  |

### 1.2 Flujo m√°s simple

```mermaid
sequenceDiagram
  participant User
  participant LLM
  User->>LLM: "¬øCu√°nto es 17√ó23?"
  LLM->>Tool[multiply]: a=17, b=23
  Tool-->>LLM: 391
  LLM-->>User: "17 √ó 23 es 391"
```

---

## 2. Entorno Python & configuraci√≥n

```bash
python -m venv myenv && source myenv/bin/activate   # Windows: myenv\Scripts\activate
pip install -U langchain langchain-google-genai python-dotenv pydantic
```

`.env`:

```env
GOOGLE_API_KEY=TU_CLAVE_GOOGLE
LANGSMITH_TRACING=true
LANGSMITH_API_KEY=opcional
LANGSMITH_PROJECT=default
```

---

## 3. Hello‚ÄëWorld: Gemini ChatBox con Tools

1. Define `add_func` y `multiply_func`.
2. Publ√≠calas via `Tool.from_function`.
3. Inicializa modelo:

```python
llm = init_chat_model("gemini-2.0-flash", model_provider="google_genai")
llm_with_tools = llm.bind_tools(tools)
```

4. Bucle interactivo: historia `chat_history` + detecci√≥n `response.tool_calls`.
5. Ejecuta tool y devuelve resultado.

### 3.1 C√≥digo completo simplificado

*(para consultas sueltas, sin conversaci√≥n continua)*

```python
# versi√≥n minimal (solo invoke simple)
# ... (se mantiene)
```

### 3.2 C√≥digo completo **interactivo** con Tools + Memoria

Este script te permite conversar libremente con Gemini, mantener historial y ejecutar
herramientas matem√°ticas en mitad del di√°logo.

```python
import os
from dotenv import load_dotenv
from langchain_core.tools import Tool
from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, AIMessage

# 1 ‚îÄ Cargar .env
a
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY no est√° definida")

# 2 ‚îÄ Config LangSmith (opcional)
os.environ.setdefault("LANGSMITH_TRACING", "true")
os.environ.setdefault("LANGSMITH_PROJECT", "default")

# 3 ‚îÄ Tools (add & multiply)

def add_func(a: float, b: float) -> float:
    print(f"‚ûï {a} + {b} = {a + b}")
    return a + b

def multiply_func(a: float, b: float) -> float:
    print(f"‚úñÔ∏è {a} * {b} = {a * b}")
    return a * b

class AddInput(BaseModel):
    a: float = Field(...)
    b: float = Field(...)

class MultiplyInput(BaseModel):
    a: float = Field(...)
    b: float = Field(...)

tools = [
    Tool.from_function("add", "Suma dos n√∫meros", lambda x: add_func(**x), AddInput),
    Tool.from_function("multiply", "Multiplica dos n√∫meros", lambda x: multiply_func(**x), MultiplyInput),
]

# 4 ‚îÄ LLM + binding
llm = init_chat_model("gemini-2.0-flash", model_provider="google_genai")
llm_with_tools = llm.bind_tools(tools)

tool_map = {"add": add_func, "multiply": multiply_func}
chat_history = []  # almacena HumanMessage / AIMessage

print("
ü§ñ Gemini ChatBot activo ‚Äî escribe 'salir' para terminar
")
while True:
    user = input("üßë T√∫: ").strip()
    if user.lower() in {"salir", "exit", "quit"}:
        break

    chat_history.append(HumanMessage(content=user))
    response = llm_with_tools.invoke(chat_history)

    if getattr(response, "tool_calls", None):
        print("üõ† Herramienta(s) solicitada(s):")
        for call in response.tool_calls:
            name, args = call["name"], call["args"]
            result = tool_map[name](**args)
            print(f"‚úÖ {name}: {result}")
            chat_history.append(AIMessage(content=f"Resultado de {name}: {result}"))
    else:
        print("ü§ñ", response.content)
        chat_history.append(AIMessage(content=response.content))
```

*Puntos clave*: historial de conversaci√≥n, detecci√≥n de `tool_calls`, impresi√≥n de resultados y memoria viva.

---

## 4. Router Chains: m√∫ltiples expertos

### 4.1 Por qu√©

* Rutea preguntas al experto adecuado (F√≠sica, Matem√°ticas, Historia‚Ä¶).
* Mantiene prompts especializados y resultados coherentes.

### 4.2 Plantilla de Router

```python
router_chain = LLMRouterChain.from_llm(llm, router_prompt)
multi_chain = MultiPromptChain(router_chain, destination_chains, default_chain)
```

### 4.3 Ejemplo de dominios

| Nombre      | Prompt resumido                 |
| ----------- | ------------------------------- |
| **physics** | "You are a physics professor‚Ä¶"  |
| **math**    | "You are a mathematician‚Ä¶"      |
| **history** | "You are a historian‚Ä¶"          |
| **cs**      | "You are a computer scientist‚Ä¶" |

---

## 5. Agentes (ReAct, MRKL, Plan‚Äë&‚ÄëExecute)

* **initialize\_agent** para crear agentes reactivos.
* Ciclo *Thought ‚Üí Action ‚Üí Observation* permite reasoning step‚Äëby‚Äëstep.

```python
from langchain.agents import initialize_agent, AgentType
agent = initialize_agent(tools, llm, agent_type=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION)
```

---

## 6. Memoria avanzada

| Tipo                       | Caso de uso                           | Ventajas                      |
| -------------------------- | ------------------------------------- | ----------------------------- |
| Buffer                     | Chat corto (<4k tokens).              | Sencillo.                     |
| Window                     | Enfoque en los √∫ltimos N turnos.      | Evita contexto infinito.      |
| Summary                    | Conversaciones largas (>10k tokens).  | Mantiene gist + reduce coste. |
| TokenBuffer                | Garantiza no exceder l√≠mite.          | Control granular.             |
| RunnableWithMessageHistory | API moderna, soporta storage externo. | Escalable.                    |

---

## 7. LangSmith en producci√≥n

1. Crea proyecto: `LANGSMITH_PROJECT=my_chatbot`.
2. Activa tracing y analytics.
3. Ver en dashboard: latencia, coste, success rate.
4. Realiza test A/B entre prompts.

---

## 8. Buenas pr√°cticas

* **Nunca** hardcodees claves ‚Üí usa `.env`.
* Mant√©n `langchain` actualizado y revisa changelog.
* Usa IDs de modelo vigentes (`gemini-2.0-flash`, `gpt-4o-mini`, etc.).
* Valida la entrada con `Pydantic` para evitar fallos en tools.
* Loguea con LangSmith desde el d√≠a 1.

---

## 9. Escalado y despliegue

| Meta                | Recomendaci√≥n                              |
| ------------------- | ------------------------------------------ |
| **Persistencia**    | Redis (chat\_history), Postgres (RAG docs) |
| **Retrieval (RAG)** | Pinecone, Chroma, FAISS                    |
| **API**             | FastAPI + LangServe                        |
| **Frontend**        | Streamlit, Next.js + LangChain.js          |
| **Infra**           | Docker + CI/CD + monitoring                |

---

## 10. Roadmap de maestr√≠a

1. **Fundamentos**: dominar prompts, tools, memory.
2. **Agentes**: ReAct, toolkits, plan & execute.
3. **Router/Graph**: flujos multi‚Äêagente con LangGraph.
4. **RAG avanzado**: embeddings, clasificadores, re‚Äërankers.
5. **Observabilidad**: m√©tricas, evaluaci√≥n continua.
6. **Escalabilidad**: microservicios, serverless, shards de vectores.

---

### üìö Recursos recomendados

* **Docs LangChain**: [https://python.langchain.com/](https://python.langchain.com/)
* **Gu√≠a Gemini**: [https://ai.google.dev/docs](https://ai.google.dev/docs)
* **Curso Agents & RAG** (LangChain University).
* **Blog LangSmith** para observabilidad.

---

## üèÅ Conclusi√≥n

Con esta gu√≠a tienes todo el itinerario: desde instalar LangChain hasta desplegar un agente multi‚Äëherramienta monitorizado en producci√≥n. Practica cada bloque, integra herramientas nuevas y experimenta con LangGraph para flujos complejos.

> **Pr√≥ximo paso:** clona el repo de ejemplo, a√±ade divisiones y potencias como tools y mide trazas en LangSmith.

