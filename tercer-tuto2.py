import os
from dotenv import load_dotenv
from langchain_core.tools import Tool
from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, AIMessage

# Cargar variables de entorno
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY no est√° definida")

# Configuraci√≥n de LangSmith (opcional)
os.environ["LANGSMITH_TRACING"] = os.getenv("LANGSMITH_TRACING", "true")
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
os.environ["LANGSMITH_PROJECT"] = os.getenv("LANGSMITH_PROJECT", "default")

# Definir funciones matem√°ticas
def add_func(a: float, b: float) -> float:
    print(f"‚ûï Ejecutando suma: {a} + {b} = {a + b}")
    return a + b

def multiply_func(a: float, b: float) -> float:
    print(f"‚úñÔ∏è Ejecutando multiplicaci√≥n: {a} * {b} = {a * b}")
    return a * b

# Esquemas de entrada
class AddInput(BaseModel):
    a: float = Field(..., description="Primer n√∫mero")
    b: float = Field(..., description="Segundo n√∫mero")

class MultiplyInput(BaseModel):
    a: float = Field(..., description="Primer n√∫mero")
    b: float = Field(..., description="Segundo n√∫mero")

# Herramientas disponibles
tools = [
    Tool.from_function(name="add", description="Suma dos n√∫meros", func=lambda x: add_func(**x), args_schema=AddInput),
    Tool.from_function(name="multiply", description="Multiplica dos n√∫meros", func=lambda x: multiply_func(**x), args_schema=MultiplyInput)
]

# Modelo + herramientas
llm = init_chat_model("gemini-2.0-flash", model_provider="google_genai")
llm_with_tools = llm.bind_tools(tools)

# Mapeo de nombres a funciones reales
tool_map = {"add": add_func, "multiply": multiply_func}

# Historial de conversaci√≥n
chat_history = []

print("\nü§ñ Gemini ChatBot activo ‚Äî Escribe 'salir' para terminar\n")

while True:
    user_input = input("üßë T√∫: ").strip()
    if user_input.lower() in {"salir", "exit", "quit"}:
        break

    chat_history.append(HumanMessage(content=user_input))
    response = llm_with_tools.invoke(chat_history)

    if getattr(response, "tool_calls", None):
        print("üõ† El modelo ha solicitado ejecutar herramienta(s):")
        for call in response.tool_calls:
            name = call["name"]
            args = call["args"]
            print(f"  üîß {name}({args})")
            result = tool_map[name](**args)
            print(f"‚úÖ Resultado de {name}: {result}")
            chat_history.append(AIMessage(content=f"Resultado de {name}: {result}"))
    else:
        print("ü§ñ:", response.content)
        chat_history.append(AIMessage(content=response.content))
